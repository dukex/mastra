---
title: "Output Processors"
description: "Learn how to use output processors to intercept and modify agent responses after they're generated by the language model."
---

# Output Processors

Output Processors allow you to intercept, modify, validate, or filter messages _after_ they are generated by the language model but before they are returned to the user. This is useful for implementing response validation, content filtering, response transformation, and safety controls on AI-generated content.

Processors operate on the assistant messages returned by the language model. They can modify, filter, or validate content, and even abort the response entirely if certain conditions are met.

## Built-in Processors

Mastra provides several built-in output processors for common use cases:

### `ModerationOutputProcessor`

This processor provides content moderation using an LLM to detect inappropriate content in AI responses across multiple categories.

```typescript copy showLineNumbers {5-13}
import { ModerationOutputProcessor } from "@mastra/core/agent/output-processor/processors";

const agent = new Agent({
  outputProcessors: [
    new ModerationOutputProcessor({
      model: openai("gpt-4.1-nano"), // Use a fast, cost-effective model
      threshold: 0.7, // Confidence threshold for flagging
      strategy: 'replace', // Replace flagged content with safe alternative
      categories: ['hate', 'harassment', 'violence'], // Custom categories
      replacementText: 'I apologize, but I cannot provide that response.',
    }),
  ],
});
```

Available options:
- `model`: Language model for moderation analysis (required)
- `categories`: Array of categories to check (default: ['hate','hate/threatening','harassment','harassment/threatening','self-harm','self-harm/intent','self-harm/instructions','sexual','sexual/minors','violence','violence/graphic'])
- `threshold`: Confidence threshold for flagging (0-1, default: 0.5)
- `strategy`: Action when content is flagged (default: 'block')
- `replacementText`: Text to use when strategy is 'replace' (default: safe apology message)
- `customInstructions`: Custom instructions for the moderation agent
- `includeScores`: Include confidence scores in logs (default: false)

Strategies available:
- `block`: Reject the response with an error (default)
- `warn`: Log warning but allow content through
- `filter`: Remove flagged messages but continue processing
- `replace`: Replace flagged content with safe alternative text

## Applying Multiple Processors

You can chain multiple output processors. They execute sequentially in the order they appear in the `outputProcessors` array. The output of one processor becomes the input for the next.

**Order matters!** Generally, it's best practice to place security checks first, content validation next, and content modification last.

```typescript copy showLineNumbers {9-16}
import { Agent } from "@mastra/core/agent";
import { 
  ModerationOutputProcessor,
  // Add other processors as they become available
} from "@mastra/core/agent/output-processor/processors";

const secureAgent = new Agent({
  outputProcessors: [
    // 1. Check for policy violations first
    new ModerationOutputProcessor({ 
      model: openai("gpt-4.1-nano"),
      strategy: 'block' 
    }),
    // 2. Add more processors here as needed
  ],
});
```

## Creating Custom Processors

You can create custom output processors by implementing the `OutputProcessor` interface.

```typescript copy showLineNumbers {4-19,23-26}
import type { OutputProcessor, MastraMessageV2, TripWire } from "@mastra/core/agent";

class ResponseLengthLimiter implements OutputProcessor {
  readonly name = 'response-length-limiter';
  
  constructor(private maxLength: number = 2000) {}

  process({ messages, abort }: { 
    messages: MastraMessageV2[]; 
    abort: (reason?: string) => never 
  }): MastraMessageV2[] {
    // Process only assistant messages
    return messages.map(message => {
      if (message.role !== 'assistant') {
        return message;
      }

      try {
        const textContent = this.extractTextContent(message);
        
        if (textContent.length > this.maxLength) {
          // Truncate the response
          const truncatedText = textContent.substring(0, this.maxLength) + '...';
          
          return {
            ...message,
            content: {
              ...message.content,
              parts: [
                {
                  type: 'text',
                  text: truncatedText,
                },
              ],
            },
          };
        }
      } catch (error) {
        if (error instanceof TripWire) {
          throw error; // Re-throw tripwire errors
        }
        throw new Error(`Length validation failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
      }
      
      return message;
    });
  }

  private extractTextContent(message: MastraMessageV2): string {
    let text = '';
    if (message.content.parts) {
      for (const part of message.content.parts) {
        if (part.type === 'text' && 'text' in part && typeof part.text === 'string') {
          text += part.text + ' ';
        }
      }
    }
    if (!text.trim() && typeof message.content.content === 'string') {
      text = message.content.content;
    }
    return text.trim();
  }
}

// Use the custom processor
const agent = new Agent({
  outputProcessors: [
    new ResponseLengthLimiter(1500), // Limit responses to 1500 characters
  ],
});
```

When creating custom output processors:
- Always return the `messages` array (potentially modified)
- Use `abort(reason)` to terminate processing and block the response entirely
- Focus on assistant messages (role === 'assistant') as those are the AI responses
- Mutate the input messages directly, make sure to mutate both the parts and content of a message
- Keep processors focused on a single responsibility
- If using an agent inside your processor, use a fast model and make prompts concise for better performance

## Integration with Agent Methods

Output processors currently work **only with `streamVNext()`** for real-time chunk processing. They process each text chunk as it streams to the frontend, ensuring content is filtered or modified before reaching the user.

```typescript copy showLineNumbers
// Output processors work with streamVNext() - processing each chunk in real-time
const agent = new Agent({
  name: 'streaming-agent',
  instructions: 'You are a helpful assistant',
  model: openai('gpt-4o'),
  outputProcessors: [
    new ModerationOutputProcessor({
      model: openai('gpt-4o-mini'),
      strategy: 'replace',
      replacementText: '[FILTERED]',
    }),
  ],
});

const stream = agent.streamVNext('Tell me about...');

for await (const chunk of stream) {
  if (chunk.type === 'text-delta') {
    // This text has already been processed by output processors
    console.log(chunk.payload.text); // Filtered/modified content
  }
}
```

If any processor blocks a text chunk, that specific chunk is filtered out (replaced with empty text), but the stream continues. This allows for fine-grained control over streaming content without interrupting the entire response.

### Why Only streamVNext?

Output processors are designed for real-time streaming scenarios where content needs to be processed **as it's being generated**, not after the fact. This is particularly important for:

- **Content moderation**: Preventing harmful content from reaching users in real-time
- **Progressive filtering**: Modifying content as it streams rather than waiting for completion
- **Responsive UX**: Users see clean, processed content immediately

For non-streaming use cases (`generate()`), you can process the complete response after generation using standard post-processing techniques.

## Response Modification

Output processors can modify the response text, which will be reflected in the final result:

```typescript copy showLineNumbers {12-25}
class ProfanityFilter implements OutputProcessor {
  readonly name = 'profanity-filter';

  process({ messages }: { messages: MastraMessageV2[] }): MastraMessageV2[] {
    return messages.map(message => {
      if (message.role !== 'assistant') {
        return message;
      }

      const textContent = this.extractTextContent(message);
      const cleanedText = this.replaceProfanity(textContent);

      if (cleanedText !== textContent) {
        return {
          ...message,
          content: {
            ...message.content,
            parts: [
              {
                type: 'text',
                text: cleanedText,
              },
            ],
          },
        };
      }

      return message;
    });
  }

  private replaceProfanity(text: string): string {
    // Your profanity filtering logic here
    return text.replace(/badword/gi, '***');
  }
}
```

## Comparison with Input Processors

| Aspect | Input Processors | Output Processors |
|--------|------------------|-------------------|
| **When they run** | Before LLM call | During streaming (chunk-by-chunk) |
| **What they process** | User messages | Assistant message chunks |
| **Methods supported** | `generate()`, `stream()`, `streamVNext()` | `streamVNext()` only |
| **Processing mode** | Full message batch | Real-time chunk processing |
| **Use cases** | Content moderation, prompt injection detection, PII filtering | Real-time content filtering, progressive moderation |
| **Performance impact** | Delays LLM call | Processes chunks as they stream |
| **Blocking behavior** | Prevents LLM call entirely | Filters individual chunks |